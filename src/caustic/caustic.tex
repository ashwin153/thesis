\documentclass[../main.tex]{subfiles}

\begin{document}
In this chapter, we will discuss Caustic. Caustic is a transactional programming language that
allows programs to be written as if they were single-threaded applications, but permits them to be
distributed arbitrarily without error. We will first motivate transactional programming by
discussing race conditions and the various techniques that are typically used to deal with them. We
will then introduce the Caustic programming language and discuss the various assumptions that it
makes about the underlying storage system and the implementation of its various components. We will
conclude with an evaluation of its programmability and performance.

% Race Conditions.
\section{Race Conditions}
Race conditions may occur whenever the order of execution of concurrent programs affects their
outcome. For example, suppose there exist two programs $A$ and $B$ that each increment a shared
counter $x$. Each program must first read the current value of $x$ in order to write $x + 1$. If
$B$ reads \emph{after} $A$ writes, then $B$ reads $x + 1$ and writes $x + 2$. However, if $B$ reads
\emph{before} $A$ writes but after $A$ reads, then both $A$ and $B$ will read $x$ and write $x + 1$.
This is an example of a race condition, because the value of the counter $x$ depends on the order in
which $A$ and $B$ are executed. This race condition may seem relatively benign, but it can have
catastrophic consequences in practical systems. Suppose the value of $x$ corresponded to your bank
balance. What if your bank determined your balance differently depending on the order in which
deposits are made? Race conditions manifest themselves in subtle ways in concurrent systems, and
they can often be difficult to detect and challenging to remove.

Before introducing mechanisms to deal with race conditions, we must first define the necessary and
sufficient criteria that any such mechanism must satisfy to correctly mitigate race conditions.
Race conditions are a relatively well-studied problem in database literature, and we may draw upon
known results from the field to define four properties of race-free programs.~\cite{transactions}

\begin{itemize}
\item \textbf{Atomic}: Programs are all-or-nothing; they are never partially applied.
\item \textbf{Consistent}: Programs see the effect of all completed programs.
\item \textbf{Isolated}: Programs cannot see the effect of in-progress programs.
\item \textbf{Durable}: The effects of completed programs are permanently visible.
\end{itemize}

  \subsection{Locks and Leases}
  A common approach to dealing with race conditions is locking. Before a program accesses or
  modifies shared data, it first declares its intentions to other programs by acquiring a
  lock that it subsequently releases when it is finished with the data. Locks trivially satisfy
  the ACID criteria, because they require programs to have exclusive ownership before performing any
  operations on shared data. However, locking introduce new problems.

  First, concurrent acquisition of multiple locks can cause a deadlock that prevents the system from
  making progress. For example, if program $A$ acquires lock $x$ and then attempts to acquire $y$
  and program $B$ acquires $y$ and then attempts to acquire $x$, then neither $A$ nor $B$ can make
  progress because each is waiting for the other to release their lock. This problem is typically
  mitigated by imposing a total order on lock acquisition; for any two locks $x$ and $y$, $x$ will
  always be acquired before $y$ or $y$ will always be acquired before $x$.

  Second, faulty programs may never release their locks. For example, if program $A$ acquires lock
  $x$ and subsequently fails, then no other program can ever acquire $x$. This problem is typically
  mitigated by using leases. \cite{leases} A lease is a lock that is automatically released after a
  certain amount of time. Programs must be carefully constructed so that they complete their
  operations on shared data within the lease duration or refresh the lease before it expires.

  Third, locking is expensive. Locks must be acquired regardless of whether or not there actually
  are concurrent operations on shared data, because programs cannot know if there are or will be
  other programs that want to simultaneously use the data. This significantly degrades
  performance in situations where contention between programs is low.

  Fourth, locking cannot protect against programmer error. Programmers may omit or incorrectly use a
  lock and thereby introduce race conditions into their program. Locks cannot guarantee they will be
  used correctly, and, therefore, cannot guarantee that race conditions will be exhaustively
  eliminated from a program.

  \subsection{Database Transactions}
  An alternative approach is to use a transactional database to protect shared data from concurrent
  modification. There are a number of storage systems that provide ACID transactions in full or in
  limited capacity. However, each of these storage systems has their own bespoke interface for
  specifying transactions that are often lacking in functionality and performance. Recent years have
  marked a proliferation in NoSQL databases that scale well by shedding functionality. These
  databases were not popularized because of their query languages, they were \emph{in spite} of
  them. Some, like Cassandra and Aerospike, attempt to mimic the relational semantics of SQL, but
  they fall short of implementing the entire SQL specification. Others, like MongoDB and DynamoDB,
  implement entire new query languages. Even SQL is not beyond reproach. SQL lacks a canonical
  implementation and has ambiguous and unintuitive syntax. \cite{sql} Relational databases like
  MySQL and PostgreSQL that each claim to implement the same SQL specification actually implement
  incompatible subsets of its functionality. Stark differences between query languages tightly
  couples storage systems and the programs that are run on them, and makes the choice of database
  in an application effectively permanent. While transactional storage systems provide the necessary
  guarantees on which correct distributed systems can be built, their collective lack of a robust
  and uniform interface makes it all but impossible to design non-trivial applications.

  \subsection{Optimistic Concurrency}
  Optimistic concurrency allows multiple programs to simultaneously access, but not modify, shared
  data without acquiring locks. Each program locally buffers any modifications that it makes and
  attempts to atomically apply the modifications when it completes conditioned on the data that it
  accessed remaining unchanged. If any data was modified, the program retries. This conditional
  update, known as a multi-word compare-and-swap and referred to as a \textbf{transaction}, is known
  to satisfy the ACID criteria and is widely used in a number of software transactional memory
  systems including Caustic. \cite{stm} Optimistic concurrency assumes that contention between
  programs will be low, because frequent retries can significantly degrade performance.


% Design.
\section{Design}
Caustic is composed of three components: a \textbf{runtime} that executes programs, a
\textbf{standard library} that simplifies program construction, and a \textbf{compiler} that
simplifies program expression. In this section, we'll describe each of these components in detail.

  % Assumptions.
  \subsection{Assumptions}
  Caustic assumes that the underlying storage \textbf{volume} is a key-value store that supports two
  operations: non-transactional \emph{get} and transactional \emph{cas}. The key challenge in
  implementing cas is detecting whether or not dependencies have changed. Caustic uses multi-version
  concurrency controls to track modifications to keys. It associates each key with
  \textbf{revision}, or versioned value, whose version is incremented each time its value is
  changed. We say that revision $A$ \emph{conflicts} with revision $B$ if $A$ and $B$ correspond to
  the same key and the version of $A$ is less than $B$. Conflict is an asymmetric relation; if $A$
  conflicts with $B$, then $B$ does not conflict with $A$.

  Formally, we require that get returns any revisions for a set of keys and that cas transactionally
  updates the revisions of a set of keys if and only if the revisions of a set of dependent keys do
  not conflict with their corresponding revisions in the volume. We will provide a distributed,
  fault-tolerant implementation of a volume in Beaker. For now, we will assume that such an
  implementation exists.

  \subsection{Runtime}
  The runtime is a virtual machine that dynamically translates \textbf{programs} into transactions.
  A program is an abstract-syntax tree that is composed of \textbf{literals} and
  \textbf{expressions}. A literal is a scalar value of type flag, real, text, and null which
  correspond to bool, double, string, and null respectively in most C-style languages. An expression
  is a function that transforms literal arguments into a literal result. Expressions may be chained
  together arbitrarily to form complex programs. Table~\ref{table:expressions} enumerates the
  various expressions natively supported by the runtime.

    \subsubsection{Execution}
    The runtime uses iterative partial evaluation to gradually reduce programs into a single literal
    result. Given correct implementations of get and cas, the runtime executes programs according to
    the following procedure.

    \begin{enumerate}
      \item \textbf{Fetch}: Get all keys that are read or written by the program that have not been
            fetched before and add the returned revisions to a local snapshot.
      \item \textbf{Evaluate}: Recursively replace all expressions with literal arguments with their
            corresponding literal result. For example,

            \[
            \begin{gathered}
            add(real(1), sub(real(0), real(2))) \rightarrow real(-1)
            \end{gathered}
            \]

            The result of all writes is saved to a local buffer and the result of all read
            expressions is the latest value of the key in the local buffer or snapshot.
      \item \textbf{Repeat}: Loop until the program is reduced to a single literal. Because all
            expressions with literal arguments return a literal result, all programs will eventually
            reduce to a single literal.
      \item \textbf{Commit}: Cas all keys in the local buffer conditioned on all revisions in
            the local snapshot. Because programs are executed with snapshot isolation, they are
            guaranteed to be serializable. Serializability implies that concurrent execution has the
            same effect as some sequential execution, and, therefore, that program execution will be
            robust against race conditions.
    \end{enumerate}

    \subsubsection{Optimizations}
    First, execution is tail-recursive. Therefore, programs may be composed of arbitrarily many
    nested expressions without overflowing the stack frame. This also allows the Scala compiler is
    able to aggressively optimize execution into a tight loop. Second, the runtime batches I/O.
    Reads are performed simultaneously whenever possible and writes are buffered and simultaneously
    committed. By batching I/O, the runtime performs a minimal number of operations on the database.
    This has significant performance implications, because I/O overhead is overwhelmingly the
    bottleneck by many orders of magnitude. \cite{io}

  \subsection{Standard Library}
  The runtime provides native support for an extremely limited subset of the operations that
  programmers typically rely on to write programs. The standard library supplements the
  functionality of the runtime by exposing a rich Scala DSL complete with static types, records,
  math, collections, and control flow.

    \subsubsection{Typing}
    The runtime natively supports just four dynamic types: $flag$, $real$, $text$, and $null$.
    Dynamic versus static typing is a religious debate among programmers. Advocates of dynamic
    typing often mistakenly believe that type inference and coercive subtyping cannot be provided by
    a static type system. In fact, they can. Because static type systems are able to
    detect type inaccuracies at compile-time, they allow programmers to write more concise and
    correct code. \cite{typing} The standard library provides rich static types and features
    aggressive type inference and subtype polymorphism.

    The standard library supports four $Primitive$ types. In descending order of precedence, they
    are: $String$, $Double$, $Int$, $Boolean$. A $Value[+T <: Primitive]$ represents a scalar value.
    Values are covariant in $T$; a $Value[Int]$ is a $Value[Double]$, but a $Value[String]$ is
    not a $Value[Boolean]$. Values may either be $Constant$ or $Variable$. A $Constant$ corresponds
    to an immutable quantity, and a $Variable$ corresponds to a value that is either stored locally
    in memory or remotely in the database.

    \subsubsection{Records}
    In addition to these $Primitive$ types, the standard library also supports references to
    user-defined types. A $Reference[T]$ is a reference to an object of type $T$. References use
    \href{https://github.com/milessabin/shapeless}{Shapeless} to materialize compiler macros that
    permit the fields of $T$ to be statically manipulated and iterated. A current limitation is that
    objects cannot be self-referential; an object cannot have a field of its own type.

    \subsubsection{Math}
    The runtime natively supports just nine mathematical operations: $add$, $sub$, $mul$, $div$, $pow$,
    $log$, $floor$,$sin$, and $cos$. However, these primitive operations are sufficient to derive the
    entire Scala \href{https://www.scala-lang.org/api/2.12.1/scala/math/index.html}{math} library using
    various mathematical identities and Taylor series approximations. The $div$, $log$, $sin$, and $cos$
    functions can actually be implemented in terms of the other primitive operations; however, native
    support for them was included in the runtime to improve performance. The standard library provides
    implementations for the functions enumerated in Table \ref{table:math}.

    \subsubsection{Collections}
    The runtime has no native support for collections of key-value pairs. The standard library
    provides implementations of three fundamental data structures: $List$, $Set$, and $Map$. These
    collections are mutable, statically-typed, and thread-safe. Collections take care of the messy
    details of mapping structured data onto a flat namespace, and feature prefetched iteration. A
    current limitation is that collections may only contain $Primitive$ types.

    \subsubsection{Control Flow}
    The runtime has native support for control flow operations like $branch$, $cons$, and $repeat$.
    However, it is syntactically challenging to express these contructs. The standard library
    uses structural types to provide support for $If$, $While$, $Return$, $Assert$, and $Rollback$.
    The standard library uses an implicitly available parsing $Context$ to track modifications made
    to variables, references, and collections and to detect when any control flow statements are
    called.

  \subsection{Compiler}
  The standard library provides additional functionality that is deficient in the runtime to make it
  easier to construct programs. However, the standard library does not address the syntactic
  challenges of expressing programs. The compiler translates programs written in the
  statically-typed, object-oriented Caustic programming language into runtime-compatible programs.
  For example, consider the following example of a distributed counter written in Caustic. This
  program may be compiled into a Scala library that may be run without modification on any
  underlying transactional storage volume and distributed arbitrary without error. The Akka project
  also provides an \href{https://git.io/vxS6u}{implementation} of a backend-agnostic distributed
  counter. Their implementation is almost seven times longer.

    \subsection{Generation}
    The compiler uses ANTLR \cite{antlr} to generate a predicated LL(*) parser from an ANTLR grammar
    file. The compiler generates code by walking the parse tree and replacing statements and
    declarations with their equivalent formulation using the standard library. The compiler mangles
    definitions so that they are lexically scoped. It also performs type inference and checking to
    determine and verify static types. The compiler is integrated into the
    \hyperref{https://www.pantsbuild.org/}{Pants} build system and provides a
    \hyperref{https://macromates.com/}{TextMate} bundle that implements syntax highlighting and code
    completion for most text editors and IDEs.


% Evaluation.
\section{Evaluation}

% Conclusion.
\section{Conclusion}


\end{document}